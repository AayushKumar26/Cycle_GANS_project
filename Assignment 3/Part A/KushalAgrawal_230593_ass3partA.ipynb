{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"165YxUKM1I27bKccQPKEJJ3wKJnAEtsId","timestamp":1716909639859}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ASSIGNMENT 3\n","PART A consists of theoretical questions. Objective answers will receive zero marks. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading and finding about them). Questions that need less explanation can be answered in 95-150 words. Please ensure the answers are well-written and thorough."],"metadata":{"id":"wg6hsKMq398n"}},{"cell_type":"markdown","source":["# PART A"],"metadata":{"id":"OFQN0GPrnSd3"}},{"cell_type":"markdown","source":["Q 1)What are optimizers in ML. Give some examples"],"metadata":{"id":"OuKQGaqx2a5i"}},{"cell_type":"markdown","source":["Optimizers are different algorithms or methods in machine learning that are used to change the attributes of the neural network such as weights, bias and learning rate to reduce the loss function and increase the efficiency of model while predicting output from the given inputs. While training the deep learning model, at each epock it modifies weights and minimize the loss function so as to increase the efficieny and accuracy of the model .\n","\n","The different well known examples of optimizers are Gradient Descent, Stochastic Gradient Descent, Stochastic Gradient descent with momentum, Mini-Batch Gradient Descent, Adagrad, RMSProp, AdaDelta, and Adam."],"metadata":{"id":"WMnqxGuT-w17"}},{"cell_type":"markdown","source":["Q 2)Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"],"metadata":{"id":"X8GPK5j_2n3P"}},{"cell_type":"markdown","source":["The main difference in these three algorithms is in number of data points they use in each iteration. Gradient descent uses all the training examples in each iteration. Stochastic gradient descent uses just one random training example in each iteration. Finally Mini batch gradient descent uses a random set or batch, as the name suggests, of training examples in each iteration.\n","As gradient descent uses all the training examples it is slow algorith but it give very accurate result. Shortly it is slow and requires more memory but very much accurate. Stochstic gradient descent is very fast and is very efficient for big datasets. But it is not steady like gradient descent. It may overshoot.\n","Mini batch gradient descent is a kind of balace of the two.It calculates the gradient of the cost function using a small random subset ,which is called mini batch, of the data. Thus it is fast as well as very accurate. Although the preference of algorithm depends on our work but balaced algorithm like mini batch gradient descent is more preferred.\n"],"metadata":{"id":"a8CW3q9f_Zaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"zOFW02If-z07"}},{"cell_type":"markdown","source":["Q 3)Explain about Adam optimizer in detail"],"metadata":{"id":"0AG5Rm7c2sAO"}},{"cell_type":"markdown","source":["The Adam optimizer is basically combination of two other popular optimizers which are AdaGrad (Adaptive Gradient Algorithm) and RMSProp (Root Mean Square Propagation). It is advanced optimization algorithm and is popular because of its computational efficiency and less memory requirement.It is commonly used in training deep neurals networks like CNNs and RNNs.\n","Adam optimizer involves a combination of two methods:\n","Momentum:\n","This algorithm is used to accelerate the gradient descent algorithm by taking into consideration the exponentially weighted average of the gradients. Using averages makes the algorithm converge towards minimum value quickly.\n","Root Mean Square Propagation (RMSP):\n","Root mean square prop is a learning algorithm that tries to improve AdaGrad by taking the ‘exponential moving average’,instead of taking the cumulative sum of squared gradients like in AdaGrad.\n","Being a hybrid of above two algoriths adam optimizer has strenghts and positive qualities of both the algorithms.\n","Here, we control the rate of gradient descent in such a way that there is minimum oscillation when it reaches the global minimum while taking large steps so as to pass the local minima in the process. Therefore it combines the characteristics of the above two methods to reach the global minimum efficiently."],"metadata":{"id":"2_yNI0vYHqkV"}},{"cell_type":"markdown","source":[],"metadata":{"id":"oQhiE_oi-ejU"}},{"cell_type":"markdown","source":["Q 4)Explain the difference between Rmsprop and Adam."],"metadata":{"id":"b0I65BBb2w84"}},{"cell_type":"markdown","source":["Adam includes bias correction to account for the initialization bias in the moment estimates. This helps make the updates more stable and reliable, leading to faster convergence. RMSProp does not include bias correction, which can make it slightly less stable compared to Adam. RMSProp has less computational requirements and uses less memory since it only maintains the second moment of the gradients. On the other hand, Adam requires more computational resources and memory as it maintains both first and second moments, along with bias correction terms. Adam achieves faster convergence due to its\\ moment estimates and bias correction making it more stable and efficient for complex models and large datasets. RMSProp provides moderate convergence speed and stability but may not be as robust as Adam in handling noisy or sparse gradients. In summary, RMSProp is a simpler and more efficient algorithm that works well for many simple situations while Adam provides more robust and reliable updates leading to better performance in more complex applications."],"metadata":{"id":"ytSSKj8PMq-u"}},{"cell_type":"markdown","source":[],"metadata":{"id":"E3G3zSl5LGJ-"}},{"cell_type":"markdown","source":["Q 5)What do you think is the best optimizer among all and Why? If you cannot come to conclusive answer, you must list them all and tell in which scenario the one is preferred.Also tell the disadvantages."],"metadata":{"id":"qVYyIYOe2w_k"}},{"cell_type":"markdown","source":["Choosing the best optimizer depends on the specific situation and the nature of the problem to be solved. Hereis the list several popular optimizers illustrating when each one is preferred and what are their disadvantages.\n","\n","Stochastic Gradient Descent(SGD):It is preferred in simple problems. It is easy to apply and is computationally fast and efficient. But it may stuck in local minima which will be a problem in finding global minima.A learning rate that is too high can cause the algorithm to diverge while a learning rate that is too low can make the training process very slow.Because SGD updates the model parameters using only one or a few samples at a time the updates can have high variance. This noise can lead to a less stable convergence.\n","\n","ADAM: It is preferred for complex models with large datasets.It tackles the problem of noisy gradients.It ombines the benefits of AdaGrad and RMSProp.\n","Its major disadvantage that itis more computationally intensive.It nvolves more hyperparameters which may be sensitive to tuning.\n","\n","AdaGrad: It is preferred in problems with sparse gradients. It efficiently adaptively scales the learning rate performing well for sparse data. Its major disadvantage is thatLearning rate can become extremely small leading to convergence.\n","\n","RMSProp: It is preferred in problems with non stationary objectives such as RNNs. It is efficient for handling sparse gradients. Its major disadvantage is that it only considers the variance of the gradients which might limit its performance in some cases.\n","\n","\n"],"metadata":{"id":"eHRYINSePXAy"}},{"cell_type":"markdown","source":["Q 6)What is overfitting and underfitting"],"metadata":{"id":"AKxnf5et2xDF"}},{"cell_type":"markdown","source":["\n","Overfitting occurs when a machine learning model learns to perform exceptionally well on the training data but fails to predict correct output to test data. It happens when the model accept random fluctuations in the training data istead of finding general patterns in data. This leads to poor performance on new data because the model has essentially memorized the training examples instead of learning the relationship. Overfitting is like memorizing answers to specific questions without understanding the concepts which makes it challenging to answer new questions correctly.\n","Underfitting happens when a machine learning model is too simple to capture the general pattern of the data. It occurs when the model is not complex enough to capture the true relationship between the features and the target variable. As a result model performs poorly not only on the training data but also on test data. Underfitting is result of oversimplifying a problem where the model's predictions have high error because it fails to understand complexities of the data.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"HpeySCK6Tl6-"}},{"cell_type":"markdown","source":["Q 7)Explain the vanishing and exploding gradients\n"],"metadata":{"id":"-UCncA8M2xF5"}},{"cell_type":"markdown","source":["Vanishing Gradients: In deep neural networks vanishing gradients occur when the gradients of the loss function with respect to the model parameters become extremely small as they are propagated back through many layers during training. This phenomenon is a problem in deep networks with many layers where gradients diminish exponentially with each layer. As a result the weights in early layers are updated very slowly, leading to slow convergence and  Vanishing gradients prevent the network from effectively learning meaningful representations from the data limiting its ability to capture complex relationships.\n","\n","Exploding Gradients: On the other hand exploding gradients happen when the gradients become extremely large during backpropagation  due to unstable training conditions or exctremely high learning rates. This leads to unstable updates to the model parameters causing the loss function to oscillate or diverge. Exploding gradients can result in numerical overflow where the values of the gradients and model parameters become too large to be represented accurately by the computer leading to NaNs or infinities. This instability makes training difficult  as the optimization process becomes unpredictable.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"UrH_tWmeVT17"}},{"cell_type":"markdown","source":["Q 8)Explain batch and layer normalization in detail."],"metadata":{"id":"fNW8Hmd52xIn"}},{"cell_type":"markdown","source":["Batch Normalization: Batch normalization is a technique used in machine learning to improve the training of deep neural networks. It works by normalizing the inputs of each layer to have a mean of zero and a standard deviation of one stabilizing the training process. During training batch normalization calculates the mean and standard deviation of each feature in the mini batch and normalizes the features. This normalization step helps to reduce internal covariate shift which is the change in the distribution of the inputs to each layer as the parameters of the previous layers are updated. By normalizing the inputs batch normalization helps to stabilize the training process allowing for faster convergence and better generalization performance.\n","\n","Layer Normalization: Layer normalization is similar technique to batch normalization but instead of normalizing across mini batches layer normalization normalizes the inputs of each layer across the features dimension. For each training example the mean and standard deviation are calculated across all the features in the input instead of across the mini batch. Layer normalization helps to remove the vanishing and exploding gradient problems by ensuring that the inputs to each layer have a consistent distribution regardless of the mini batch size. It also makes the model more robust to variations  improving its generalization performance."],"metadata":{"id":"mvZfnoihWgvG"}},{"cell_type":"markdown","source":["Q 9)What are regularization techniques in machine learning?(200 words)"],"metadata":{"id":"6pMnZ6z52xLa"}},{"cell_type":"markdown","source":["Regularization techniques in machine learning are methods used to prevent overfitting which occurs when a model learns to fit the training data too closely leading to poor performance on test data. These techniques aim to make a balance between fitting the training data well and generalizing to new data .\n","One common regularization technique is L2 regularization also known as weight decay which adds a penalty term to the loss function based on the squared magnitude of the model's weights. This penalty discourages large weight making the model less sensitive to small changes in the input data and reducing overfitting.Another technique is L1 regularization, which adds a penalty term based on the absolute magnitude of the weights. L1 regularization encourages sparsity in the model by making some of the weights zero selecting only the most important features and reducing the model's complexity. Dropout is a popular regularization technique used in neural networks. During training, dropout randomly sets a fraction of the neurons to zero removing them from the network temporarily. This forces the network to learn redundant representations of the data, making it more robust and preventing overfitting.Regularization techniques help to improve the generalization performance of machine learning models by controlling their complexity and reducing the likelihood of overfitting ultimately leading to more reliable and accurate predictions on test data."],"metadata":{"id":"mTdd4SZsYALX"}},{"cell_type":"markdown","source":["Q 10)What is dropout layer and explain how it prevents overfitting?"],"metadata":{"id":"EoVZoLpB3B7X"}},{"cell_type":"markdown","source":["Dropout is a regularization technique used in neural networks to prevent overfitting. During training dropout randomly sets a fraction of the neurons in the network to zero removing them temporarily. By doing this dropout prevents the network from relying too heavily on any individual neuron or feature forcing it to learn more robust and generalized representation of the data. This randomness introduced by dropout acts as a form of ensemble learning where different subsets of neurons are trained on different parts of the data. As a result dropout helps to reduce the likelihood of overfitting by averaging out the predictions of multiple models.In short dropout promotes model robustness and improves the ability to generalize to unseen data by introducing randomness and preventing the network from memorizing the training examples."],"metadata":{"id":"xzBN1Fg1ZL2p"}},{"cell_type":"markdown","source":["Q 11)Explain L1 and L2 regularization"],"metadata":{"id":"CMx3OcJ53CBj"}},{"cell_type":"markdown","source":["L1 Regularization:\n","L1 regularization or Lasso regularization is a technique used to prevent overfitting in machine learning models. It works by adding a penalty term to the loss function based on the absolute magnitude of the model's weights. This penalty encourages sparsity in the model by driving some of the weights to exactly zero. In simpler terms L1 regularization encourages the model to select only the most important features and discard the less relevant ones. By doing so it reduces the complexity of the model and helps prevent overfitting, leading to better generalization performance on unseen data. L1 regularization is particularly useful when dealing with high-dimensional datasets where feature selection is important.\n","\n","L2 Regularization:\n","L2 regularization or Ridge regularization is another technique used to prevent overfitting in machine learning models. It works by adding a penalty term to the loss function based on the squared magnitude of the model's weights. This penalty discourages large weights in the model making it less sensitive to small changes in the input data. In simpler terms L2 regularization encourages the model to keep all the weights small rather than making them exactly zero as in L1 regularization. By doing this it reduces the model's complexity and helps prevent overfitting leading to better generalization performance on test data. L2 regularization is used in linear regression and neural networks to improve their robustness and stability."],"metadata":{"id":"i7HOiKMOZ0dN"}},{"cell_type":"markdown","source":["Q 12)Write about validation accuracy and why we need it?"],"metadata":{"id":"k5TGmHuJ3JTh"}},{"cell_type":"markdown","source":["Validation accuracy is a measure used in machine learning to evaluate the performance of a trained model on test data. During the training a portion of the available data is set as the validation set which the model does not see during training. After training the model on the training data its performance is assessed using the validation set. The validation accuracy indicates how well the model generalizes to new, test data. We need validation accuracy to ensure that the model has not simply memorized the training data but has actually learned meaningful patterns that can be applied to new inputs. By evaluating the model on a separate validation set we can assess its ability to make accurate predictions on data it has not been exposed to before. This helps us identify if the model is overfitting (performing well on the training data but poorly on test data) or underfitting (performing poorly on both training and validation data) allowing us to make changes to improve its performance and generalization of the model."],"metadata":{"id":"ZI6qZFkEaqJn"}},{"cell_type":"markdown","source":[" Q 13)What do you mean by data augmentation and explain is advantages and disadvantages in detail?"],"metadata":{"id":"hCOGr8vl3JXY"}},{"cell_type":"markdown","source":["Data augmentation is a technique used in machine learning to artificially increase the size of a training dataset by applying various transformations to the existing data. These transformations include rotation, flipping, scaling, cropping, and adding noise to the images or samples. The goal of data augmentation is to create additional variations of the original data, which helps improve the model's performance and generalization ability.\n","\n","Advantages:\n","\n","Increased Robustness: Data augmentation exposes the model to a wide range of variations in the input making it more robust to different situations and conditions it may face.\n","Reduced Overfitting: By providing the model with more diverse examples during training data augmentation helps prevent overfitting by reducing the model's dependence on specific features or patterns in the training data.\n","Improved Generalization: Augmenting the training data with additional variations helps the model learn more generalized representations of the underlying patterns leading to better performance on test data.\n","Cost effective: Data augmentation allows us to generate additional training data without the need for collecting or labeling new samples making it a costeffective way to improve model performance.\n","\n","Disadvantages:\n","\n","Important Information Loss: Some augmentation techniques may introduce artificial distortions or noise that change the original information in the data leading to a loss of valuable information.\n","Computational Overhead: Performing data augmentation onthe fly during training can increase computational complexity and training time especially for large datasets or complex transformations.\n","Quality Control: Ensuring the quality and relevance of augmented data can be challenging, as some transformations may produce  irrelevant samples that could decrease the model's performance."],"metadata":{"id":"E6u8H4jSbxOl"}},{"cell_type":"markdown","source":["Q 14)What is transfer learning.Explain in detail? Mention various pre-trained model present in the community"],"metadata":{"id":"CeYHJXKh3O1x"}},{"cell_type":"markdown","source":["Transfer learning is a machine learning technique where a model trained on one task is reused or adapted for a different but related task. Instead of training a model from scratch, transfer learning leverages the knowledge gained from solving a source task to improve performance on a target task. This approach is particularly useful when labeled data for the target task is limited or expensive to acquire. Various pre trained models are available in the community each developed by different research groups or organizations providing different architectures and capabilities.\n","VGG (Visual Geometry Group),ResNet (Residual Network),\n","BERT (Bidirectional Encoder Representations from Transformers),GPT (Generative Pre-trained Transformer) etc are different examples.These models offer a starting point for various tasks enabling us to achieve good performance with less labeled data and computational resources.\n","\n"],"metadata":{"id":"rgQAm9VKdaXC"}},{"cell_type":"markdown","source":["Q 15) Explain the bias- variance tradeoff."],"metadata":{"id":"m0x7E-Cg3Vzc"}},{"cell_type":"markdown","source":["The bias variance tradeoff is a fundamental concept in machine learning that show the balance between two types of errors a model can make, bias and variance. Bias refers to the error introduced by the simplifying assumptions made by the model to approximate the true relationship between the features and the target variable. A high bias model tends to be over simple unable to capture the complexity of the patterns in the data and therefore leads to underfitting. On the other hand variance refers to the error introduced by the model's sensitivity to small fluctuations or noise in the training data. A high variance model is too flexible capturing not only the true patterns in the data but also noise resulting in overfitting. In overfitting the model performs well on the training data but fails to generalize to unseen data.\n","\n","The tradeoff arises because reducing bias often leads to an increase in variance and vice versa. Finding the right balance between bias and variance is important for developing models that generalize well to new test data. This involves selecting model complexity, regularization, and feature engineering. For example increasing the complexity of a model can help reduce bias but may also increase variance. Regularization techniques such as L1 or L2 regularization can help control variance by penalizing overly complex models. Feature engineering can help reduce bias by providing the model with more relevant information and also minimizing the risk of overfitting at the same time. ]"],"metadata":{"id":"ZKiR3_ZDeyx_"}},{"cell_type":"markdown","source":["Q 16)Assume you have dataset of patients who visited AIIMS(Delhi) between the period of 2018-2020.The datasets include features from CBC reports,IGE,weight,temp,etc.There problems were mainly classified into gastrointestinal problems, heart problems, diabetes and misc.\n","You being a experienced ML engineer, the hospital has approached you to make a model which given these features can predict the problem that the patient is suffering.You can either train a separate neural network for each of the diseases or to train a single neural network\n","with one output neuron for each disease. Which method do you prefer.Justify your answer"],"metadata":{"id":"fUU9eDCa3S2U"}},{"cell_type":"markdown","source":["\n","I will prefer training a single neural network with one output neuron for each disease. This approach allows us to know the relationships between different diseases and their corresponding features. By training a single model we can capture the common patterns among the features of different diseases leading to a more efficient learning model. Also using a single model reduces computational complexityt. Overall, this approach better way for predicting patient diseases based on the given features."],"metadata":{"id":"Lkh0XlwwgFpC"}},{"cell_type":"markdown","source":["Q 17) Assume your input data X has m observations and n0 features (shape: n0 x m), and the output layer Y has shape (n3 x m). Say that you want to have 2 hidden layers in your model: A1- (n1 x m) and A2- (n2 x m).\n","\n","(a) How many weight and bias matrices will you have? Find the shapes of each & verify if the dimensions of matrix multiplications are accurate.\n","\n","(b) If this is a Binary classification problem, what activation functions will you use for (i) the hidden layers, and (ii) the output layer. Why?\n","\n","(c) Repeat part (b) if this is a Multi-class classification problem.\n","\n","(d) Repeat part (b) if this is a Regression problem.\n","\n","-> Give proper reasoning for each part."],"metadata":{"id":"ZRbqSHnH0Cu5"}},{"cell_type":"markdown","source":["(a)For each layer in the neural network, we have a weight matrix (W) and a bias vector (b). So, for 2 hidden layers and 1 output layer we will have 3 weight matrices (W1, W2, W3) and 3 bias vectors (b1, b2, b3). The shapes of these matrices and vectors are following:-\n","\n","Weight Matrix W1: shape (n1 x n0)\n","Bias Vector b1: shape (n1 x 1)\n","Weight Matrix W2: shape (n2 x n1)\n","Bias Vector b2: shape (n2 x 1)\n","Weight Matrix W3: shape (n3 x n2)\n","Bias Vector b3: shape (n3 x 1)\n","we can see that the number of columns in each weight matrix matches the number of rows in its corresponding activation matrix.So number of dimensions are accurate.\n","(b) For a binary classification problem:\n","(i) ReLU (Rectified Linear Unit) activation function is commonly used for the hidden layers as it helps the network learn non linear relationship in the data.\n","(ii) Sigmoid activation function is used for the output layer. Sigmoid makes the output between 0 and 1, making it suitable for binary classification tasks where the output represents the probability.\n","(c)For a multi-class classification problem:\n","(i) ReLU activation function is used for the hidden layers for the same reason as in the binary classification case.\n","(ii) Softmax activation function is used for the output layer. Softmax normalizes the output into a probability distribution allowing the model to predict the probability.\n","(b) For a regression problem:\n","(i) ReLU activation function is used for the hidden layers to know the non linear relationships in the data.\n","(ii) Linear activation function is used for the output layer. Linear activation function allows the model to predict continuous values without any compression between 0 and 1 making it effective for regression."],"metadata":{"id":"QhpN8-m2hMz9"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-I1NAfeGjHuB"}}]}