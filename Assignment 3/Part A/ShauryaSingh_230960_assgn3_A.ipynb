{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 3\n",
        "PART A consists of theoretical questions. Objective answers will receive zero marks. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading and finding about them). Questions that need less explanation can be answered in 95-150 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "wg6hsKMq398n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART A"
      ],
      "metadata": {
        "id": "OFQN0GPrnSd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1)What are optimizers in ML. Give some examples"
      ],
      "metadata": {
        "id": "OuKQGaqx2a5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans1 -> Optimizers are algorithms that adjust the model's parameters to reduce errors during training. They help the model learn by updating weights to improve predictions. For example, gradient descent, momentum, etc."
      ],
      "metadata": {
        "id": "pT_j6TLsjE9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2)Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent\n"
      ],
      "metadata": {
        "id": "X8GPK5j_2n3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans2 -> Gradient Descent:\n",
        "Uses the entire dataset to compute the gradient and update the parameters.\n",
        "\n",
        "Stochastic Gradient Descent:\n",
        "Uses one data point at a time to compute the gradient and update the parameters.\n",
        "\n",
        "Mini-Batch Gradienrt Descent:\n",
        "Uses a small batch of data points to compute the gradient and update the parameters."
      ],
      "metadata": {
        "id": "zfsLDUJsj2iJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3)Explain about Adam optimizer in detail"
      ],
      "metadata": {
        "id": "0AG5Rm7c2sAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans3 -> The Adam optimizer is an advanced algorithm combining the benefits of AdaGrad and RMSprop. It adapts the learning rates for each parameter individually, which is particularly effective for handling sparse gradients and noisy problems. Adam achieves this by maintaining running averages of both the gradients and the squared gradients. Along with adjusting the learning rates dynamically, Adam incorporates momentum to smooth out updates and speed up the convergence process.\n",
        "\n",
        "The optimization process starts by initializing the parameters and then computing the gradients of the loss function with respect to these parameters. The algorithm updates the first moment estimate by taking a moving average of the gradients, and the second moment estimate by taking a moving average of the squared gradients. To correct the biases introduced by the initialization at zero, Adam applies bias correction to these estimates. Finally, the parameters are updated using the corrected moment estimates.\n",
        "\n",
        "Adam's main advantages include its efficiency with large datasets and high-dimensional data."
      ],
      "metadata": {
        "id": "zzQqWDi6knrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4)Explain the difference between Rmsprop and Adam."
      ],
      "metadata": {
        "id": "b0I65BBb2w84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSprop uses a moving average of squared gradients to normalize the learning rate.\n",
        "Adam uses both a moving average of gradients (first moment) and a moving average of squared gradients (second moment).\n",
        "\n",
        "RMSprop does not include bias correction.\n",
        "Adam includes bias correction for both moving averages, which helps in the initial stages of training.\n",
        "\n",
        "RMSprop does not explicitly incorporate momentum.\n",
        "Adam combines the momentum term by maintaining a moving average of the gradients.\n",
        "\n",
        "RMSprop adjusts parameters based on the normalized gradient using the moving average of squared gradients.\n",
        "Adam updates parameters using both the normalized gradient and momentum, applying bias correction to both averages."
      ],
      "metadata": {
        "id": "wAc-ovWVlVx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5)What do you think is the best optimizer among all and Why? If you cannot come to conclusive answer, you must list them all and tell in which scenario the one is preferred.Also tell the disadvantages."
      ],
      "metadata": {
        "id": "qVYyIYOe2w_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Gradient Descent (GD)\n",
        "Advantages: Suitable for convex problems with small to medium-sized datasets.\n",
        "Disadvantages: Computationally expensive for large datasets since it uses the entire dataset for each update.\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "Advantages: Effective for large datasets and online learning.\n",
        "Disadvantages: Updates can be noisy, leading to a more erratic convergence path.\n",
        "3. Mini-Batch Gradient Descent\n",
        "Advantages: Balances the benefits of GD and SGD; works well for most practical applications.\n",
        "Disadvantages: Requires tuning of batch size for optimal performance.\n",
        "4. Momentum\n",
        "Advantages: Enhances SGD, useful for faster convergence and reducing oscillations in deep networks.\n",
        "Disadvantages: May overshoot the minimum if the momentum term is too large.\n",
        "5. Nesterov Accelerated Gradient (NAG)\n",
        "Advantages: Similar to momentum but anticipates the future gradient, leading to more accurate updates.\n",
        "Disadvantages: Slightly more complex to implement and tune.\n",
        "6. AdaGrad\n",
        "Advantages: Effective for problems with sparse data and features.\n",
        "Disadvantages: Learning rate can become very small, leading to premature stopping.\n",
        "7. RMSpropAdvantages: Works well for non-stationary objectives and problems with varying gradients.\n",
        "Disadvantages: Does not incorporate momentum explicitly.\n",
        "8. Adam (Adaptive Moment Estimation)\n",
        "Advantages: Versatile, works well for a wide range of problems including deep learning, especially with large datasets and noisy gradients.\n",
        "Disadvantages: Can sometimes lead to overfitting, requires more memory for storing moment estimates.\n",
        "9. AdamW\n",
        "Advantages: Similar to Adam but with weight decay, useful for preventing overfitting in deep learning.\n",
        "Disadvantages: Slightly more complex and requires careful tuning of weight decay.\n",
        "10. Adadelta\n",
        "Advantages: An extension of AdaGrad that seeks to reduce the diminishing learning rate issue.\n",
        "Disadvantages: Computationally expensive due to the need for more historical gradient information."
      ],
      "metadata": {
        "id": "g6n5r2DClyVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6)What is overfitting and underfitting"
      ],
      "metadata": {
        "id": "AKxnf5et2xDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when a model learns the training data too well, including its noise and outliers. This means the model performs exceptionally well on the training data but poorly on new, unseen data. Overfitting typically happens when the model is too complex, such as having too many parameters relative to the number of training examples.\n",
        "\n",
        "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. This results in poor performance on both the training and validation/test sets. Underfitting indicates that the model cannot capture the complexity of the data."
      ],
      "metadata": {
        "id": "BGM1-CUJmZeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 7)Explain the vanishing and exploding gradients"
      ],
      "metadata": {
        "id": "-UCncA8M2xF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vanishing gradient problem occurs when the gradients of the loss function with respect to the model parameters become very small as they are propagated back through the network. This issue is most common with deep networks and certain activation functions, such as the sigmoid or tanh.\n",
        "\n",
        "The exploding gradient problem occurs when the gradients of the loss function with respect to the model parameters become excessively large as they are propagated back through the network. This issue can also occur in deep networks and can cause instability in the training process."
      ],
      "metadata": {
        "id": "MfcL565Bmj0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 8)Explain batch and layer normalization in detail."
      ],
      "metadata": {
        "id": "fNW8Hmd52xIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization is a technique used to improve the training of deep neural networks by normalizing the inputs of each layer within a mini-batch. It helps to stabilize and accelerate the learning process.\n",
        "\n",
        "Layer normalization is another normalization technique, but it normalizes the inputs across the features for each data point independently, rather than across the mini-batch."
      ],
      "metadata": {
        "id": "AugW3AD6msZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9)What are regularization techniques in machine learning?(200 words)"
      ],
      "metadata": {
        "id": "6pMnZ6z52xLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization techniques in machine learning are methods used to prevent overfitting and improve the generalization ability of models. Overfitting occurs when a model learns the training data too well, capturing noise and irrelevant patterns, which can lead to poor performance on new, unseen data. Regularization helps mitigate overfitting by adding additional constraints or penalties to the model's optimization objective, encouraging simpler and smoother solutions.\n",
        "\n",
        "Common regularization techniques include:\n",
        "\n",
        "1) L1 Regularization (Lasso): Adds the absolute value of the coefficients to the loss function.\n",
        "\n",
        "2) L2 Regularization (Ridge): Adds the squared magnitude of the coefficients to the loss function.\n",
        "\n",
        "3) Elastic Net Regularization: Combines L1 and L2 regularization, offering a balance between sparsity and smoothness.\n",
        "\n",
        "4) Dropout: Randomly drops a fraction of neurons during training, forcing the network to learn redundant representations and preventing co-adaptation of neurons.\n",
        "\n",
        "5) Early Stopping: Monitors the model's performance on a validation set during training and stops the training process when performance starts to degrade, preventing overfitting.\n",
        "\n",
        "6) Data Augmentation: Introduces variations to the training data, such as rotation, scaling, or cropping, to increase the diversity of the dataset and improve generalization.\n",
        "\n",
        "Regularization techniques play a crucial role in improving the generalization ability of machine learning models, helping them perform well on unseen data and real-world scenarios."
      ],
      "metadata": {
        "id": "2VXmvE--pHgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10)What is dropout layer and explain how it prevents overfitting?"
      ],
      "metadata": {
        "id": "EoVZoLpB3B7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dropout layer is a regularization technique commonly used in neural networks, particularly in deep learning, to prevent overfitting. During training, dropout randomly sets a fraction of the input units (neurons) to zero with a specified probability, typically between 0.1 and 0.5. This means that the output of these neurons is temporarily ignored during forward and backward passes through the network. By randomly dropping neurons, dropout forces the network to learn redundant representations of the data. This prevents the network from relying too heavily on any specific set of features or neurons, making it more robust and less prone to overfitting."
      ],
      "metadata": {
        "id": "mSr1X1jEpkPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 11)Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "CMx3OcJ53CBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 regularization, also known as Lasso regularization, adds a penalty term to the loss function proportional to the absolute value of the model's weights.\n",
        "L1 regularisation term = lambda(sum(wi))\n",
        "\n",
        "L2 regularization, also known as Ridge regularization, adds a penalty term to the loss function proportional to the square of the model's weights.\n",
        "L2 regularisation term = lambda(sum(wi^2))"
      ],
      "metadata": {
        "id": "FTJcM3jDqPn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 12)Write about validation accuracy and why we need it?"
      ],
      "metadata": {
        "id": "k5TGmHuJ3JTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation accuracy is a metric used to evaluate the performance of a machine learning model on a separate dataset called the validation set. The validation set is distinct from the training set and is used to test how well the model generalizes to unseen data.\n",
        "\n",
        "Validation accuracy provides an estimate of how well the model performs on data it hasn't seen during training. Monitoring the validation accuracy during training helps detect overfitting. Validation accuracy is used to compare different models and select the best-performing one for deployment."
      ],
      "metadata": {
        "id": "AlT_fxghqqsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q 13)What do you mean by data augmentation and explain is advantages and disadvantages in detail?"
      ],
      "metadata": {
        "id": "hCOGr8vl3JXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation is a method used in machine learning to artificially expand a dataset by applying various transformations to the existing data samples. These transformations can include rotations, translations, flips, cropping, scaling, changes in brightness, contrast, or adding noise. The goal of data augmentation is to increase the diversity of the training data, which helps improve the robustness and generalization ability of machine learning models, particularly in computer vision tasks.\n",
        "\n",
        "**Advantages of Data Augmentation:**\n",
        "\n",
        "  Data augmentation effectively increases the size of the training dataset, providing the model with more examples to learn from. A larger dataset can help prevent overfitting and improve the model's ability to capture diverse patterns in the data.\n",
        "\n",
        "  By exposing the model to a wider variety of data variations, data augmentation helps improve its ability to generalize to unseen examples.\n",
        "\n",
        "  Data augmentation acts as a form of regularization by adding noise to the training data.\n",
        "\n",
        "**Disadvantages of Data Augmentation:**\n",
        "\n",
        "  Generating augmented data samples can be computationally expensive, especially for large datasets or complex transformations. This can increase the time and resources required for training the model.\n",
        "\n",
        "  The quality of augmented data samples may not always be as high as the original data.\n",
        "\n",
        "  If not carefully designed, data augmentation can introduce biases into the training data that affect the model's predictions."
      ],
      "metadata": {
        "id": "5LaA6Dinrh-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 14)What is transfer learning.Explain in detail? Mention various pre-trained model present in the community"
      ],
      "metadata": {
        "id": "CeYHJXKh3O1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning is a machine learning technique where a model trained on one task is reused or adapted for another related task. Instead of starting the learning process from scratch, transfer learning leverages the knowledge gained from solving one problem and applies it to a different but related problem.\n",
        "\n",
        "VARIOUS PRE-TRAINED MODELS:-\n",
        "\n",
        "GPT: Generative Pre-trained Transformer is a series of language models developed by OpenAI.\n",
        "BERT: Bidirectional Encoder Representations from Transformers (BERT) is a pre-trained language model developed by Google.\n",
        "ResNet: Residual Networks are deep neural networks known for their residual blocks, including ResNet50 and ResNet101.\n",
        "SSD: Single Shot MultiBox Detector (SSD) is a fast object detection model."
      ],
      "metadata": {
        "id": "B7OTD7YruRAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 15) Explain the bias- variance tradeoff."
      ],
      "metadata": {
        "id": "m0x7E-Cg3Vzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bias-variance tradeoff arises because reducing bias typically increases variance, and vice versa.\n",
        "High Bias, Low Variance: A model with high bias and low variance is too simple and may underfit the data. It performs poorly on both the training and test data.\n",
        "Low Bias, High Variance: A model with low bias and high variance is too complex and may overfit the training data. It captures noise in the data and performs well on the training data but poorly on new, unseen data."
      ],
      "metadata": {
        "id": "nSvmpUTOvNXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 16)Assume you have dataset of patients who visited AIIMS(Delhi) between the period of 2018-2020.The datasets include features from CBC reports,IGE,weight,temp,etc.There problems were mainly classified into gastrointestinal problems, heart problems, diabetes and misc.\n",
        "You being a experienced ML engineer, the hospital has approached you to make a model which given these features can predict the problem that the patient is suffering.You can either train a separate neural network for each of the diseases or to train a single neural network\n",
        "with one output neuron for each disease. Which method do you prefer.Justify your answer"
      ],
      "metadata": {
        "id": "fUU9eDCa3S2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, I would prefer to train a single neural network with one output neuron for each disease.\n",
        "\n",
        "Training a single neural network is more efficient than training multiple separate networks. It requires less computational resources and less time for training and maintenance.\n",
        "\n",
        "A single network can learn common patterns and features across different health problems. This shared learning can improve the model's ability to generalize to new, unseen cases and make more accurate predictions.\n",
        "\n",
        "As new data becomes available or new health problems are identified, it's easier to update and expand a single model compared to managing multiple separate models."
      ],
      "metadata": {
        "id": "KqIL5MDxvf44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 17) Assume your input data X has m observations and n0 features (shape: n0 x m), and the output layer Y has shape (n3 x m). Say that you want to have 2 hidden layers in your model: A1- (n1 x m) and A2- (n2 x m).\n",
        "\n",
        "(a) How many weight and bias matrices will you have? Find the shapes of each & verify if the dimensions of matrix multiplications are accurate.\n",
        "\n",
        "(b) If this is a Binary classification problem, what activation functions will you use for (i) the hidden layers, and (ii) the output layer. Why?\n",
        "\n",
        "(c) Repeat part (b) if this is a Multi-class classification problem.\n",
        "\n",
        "(d) Repeat part (b) if this is a Regression problem.\n",
        "\n",
        "-> Give proper reasoning for each part."
      ],
      "metadata": {
        "id": "ZRbqSHnH0Cu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANS TO PART A**\n",
        "Weight Matrices:\n",
        "\n",
        "Between Input Layer (X) and First Hidden Layer (A1):  W[1](shape:n1*n0)\n",
        "\n",
        "Between First Hidden Layer (A1) and Second Hidden Layer (A2): W[2](shape:n2*n1)\n",
        "\n",
        "Between Second Hidden Layer (A2) and Output Layer (Y): W[3](shape:n3*n2)\n",
        "\n",
        "Bias Matrices:\n",
        "\n",
        "For First Hidden Layer (A1): b[1](shape:n1*1)\n",
        "For Second Hidden Layer (A2): b[2](shape:n2*1)\n",
        "For Output Layer (Y): b[3](shape:n3*1)"
      ],
      "metadata": {
        "id": "lZyVaMlsv4Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANS TO PART B**\n",
        "\n",
        "Hidden Layers: ReLU introduces non-linearity and helps the model learn complex patterns in the data.\n",
        "\n",
        "Output Layer: Sigmoid squashes the output between 0 and 1, representing the probability of the positive class. This is suitable for binary classification where we want to predict the probability of belonging to one of the two classes.\n"
      ],
      "metadata": {
        "id": "Kbge-dd-wwNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANS TO PART C**\n",
        "\n",
        "Hidden Layers: I will use ReLU for it as well due to its simplicity and effectiveness in capturing non-linear relationships in the data.\n",
        "\n",
        "Output Layer: I will use softmax activation function for it as it normalizes the output into a probability distribution over multiple classes, ensuring that the sum of the predicted probabilities for all classes equals one. This allows us to interpret the output as the probability of belonging to each class."
      ],
      "metadata": {
        "id": "wQNvFhm_xHsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANS TO PART D**\n",
        "\n",
        "Hidden Layers: ReLU activation function can be used for the hidden layers in regression problems. ReLU introduces non-linearity and helps the model capture complex relationships in the data, which is beneficial for learning patterns in regression tasks.\n",
        "\n",
        "Output Layer: Identity Function can be used used for the output layer in regression problems. Linear activation function allows the model to output continuous values without any transformation. This is suitable for regression tasks where the goal is to predict numerical values directly, without constraining the output to a specific range or distribution.\n"
      ],
      "metadata": {
        "id": "773uSSpUxlqs"
      }
    }
  ]
}