{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 3\n",
        "PART A consists of theoretical questions. Objective answers will receive zero marks. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading and finding about them). Questions that need less explanation can be answered in 95-150 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "wg6hsKMq398n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART A"
      ],
      "metadata": {
        "id": "OFQN0GPrnSd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1)What are optimizers in ML. Give some examples"
      ],
      "metadata": {
        "id": "OuKQGaqx2a5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " optimizers are  methods used to change the attributes of the neural network, such as weights and learning rates so that loss function is minimized.We have to find  parameters so that loss function is minimized and performance and accuracy of the model is maximized.Proper selection of optimizers are important for training efficient and accurate models.optimizer depends on factors like the complexity of the dataset and computational resources available.Some examples of optimizers are :\n",
        "\n",
        "Gradient Descent:  In Gradient Descent, the algorithm computes the gradient of the loss function with respect to all the training examples in the dataset.\n",
        "It then updates the weights of the model by moving in the opposite direction of the gradient to minimize the loss function.This method can be extremely time consuming, especially for large datasets, as it requires processing the entire dataset for each parameter update.\n",
        "\n",
        "Stochastic Gradient Descent (SGD):It is similar to gradient descent but  instead of computing the gradient using the entire dataset, it computes the gradient and update the weights after each data point. This can lead to faster convergence, especially for large datasets.\n",
        "\n",
        "Mini-batch Gradient Descent: It is a mix of gradient descent and SGD where dataset is divided into mini batches and computes the gradient and update the weights after each batch.\n",
        "\n",
        "Adam (Adaptive Moment Estimation):  It adapts the learning rates for each parameter based on the first and second moments of the gradients."
      ],
      "metadata": {
        "id": "jJE9jiGP8lPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2)Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "X8GPK5j_2n3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent:  In Gradient Descent, the algorithm computes the gradient of the loss function with respect to all the training examples in the dataset.\n",
        "It then updates the weights of the model by moving in the opposite direction of the gradient to minimize the loss function.This method can be extremely time consuming, especially for large datasets, as it requires processing the entire dataset for each parameter update.\n",
        "\n",
        "\n",
        "Stochastic Gradient Descent (SGD):It is similar to gradient descent but  instead of computing the gradient using the entire dataset, it computes the gradient and update the weights after each data point. This can lead to faster convergence, especially for large datasets.\n",
        "\n",
        "Mini-batch Gradient Descent: It is a mix of gradient descent and SGD where dataset is divided into mini batches and computes the gradient and update the weights after each batch."
      ],
      "metadata": {
        "id": "2rBkR54MfWHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3)Explain about Adam optimizer in detail"
      ],
      "metadata": {
        "id": "0AG5Rm7c2sAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Adam optimizer(Adaptive Moment Estimation) is an iterative optimization algorithm used to minimize the loss function during the training of neural networks.This method is really efficient when working with large problem involving a lot of data or parameters. It requires less memory and is efficient.\n",
        "\n",
        "Adam optimizer involves a combination of two gradient descent methodologies:\n",
        "Momentum:his algorithm is used to accelerate the gradient descent algorithm by taking into consideration the ‘exponentially weighted average’ of the gradients. Using averages makes the algorithm converge towards the minima in a faster pace.\n",
        "and\n",
        "Root Mean Square Propagation (RMSP):Root mean square prop or RMSprop is an adaptive learning algorithm that tries to improve AdaGrad. Instead of taking the cumulative sum of squared gradients like in AdaGrad, it takes the ‘exponential moving average’.\n",
        " Adam optimizer gives much higher performance than the previously used and outperforms them by a big margin into giving an optimized gradient descent.\n"
      ],
      "metadata": {
        "id": "ZXxPyoUoUhVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4)Explain the difference between Rmsprop and Adam."
      ],
      "metadata": {
        "id": "b0I65BBb2w84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rmsprop (Root Mean Square Prop):\n",
        "\n",
        "Focuses on adapting the learning rate for each parameter individually.\n",
        "Considers the squared gradients of a parameter over time to get a sense of its historical variability.\n",
        "Uses this information to scale down the learning rate for parameters with consistently large gradients.\n",
        "Relies on the second moment (squared gradients) of the gradients.\n",
        "Can be a good choice for problems with sparse gradients or features with significantly different scales.\n",
        "May be slightly simpler to tune due to fewer hyperparameters\n",
        "\n",
        "Adam (Adaptive Moment Estimation):\n",
        "\n",
        "Combines the ideas of Rmsprop and momentum.\n",
        "Maintains an exponentially decaying average of both the first moment (average gradient) and the second moment (uncentered variance of the gradients) for each parameter.\n",
        "Uses these averages to dynamically adjust the learning rate for each parameter.\n",
        "Utilizes both the first moment (average gradient) and the second moment (uncentered variance)\n",
        "it is used as default optimizer due to its effectiveness in a wide range of problems."
      ],
      "metadata": {
        "id": "iy_W7QoIUgPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5)What do you think is the best optimizer among all and Why? If you cannot come to conclusive answer, you must list them all and tell in which scenario the one is preferred.Also tell the disadvantages."
      ],
      "metadata": {
        "id": "qVYyIYOe2w_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6)What is overfitting and underfitting"
      ],
      "metadata": {
        "id": "AKxnf5et2xDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when a model memorizes the training data too closely, including the noise and random errors.This will lead to high accuracy on training data of the model but low accuracy of the model on the test data.They usually have high variance therefore highly sensitive towards new data.\n",
        "\n",
        "Underfitting occurs when a model is too simple and cannot learn the patterns in the training data. This results in poor performance on both the training data and test data.they have  have high bias therefore they are unable to handle the complexity of the data"
      ],
      "metadata": {
        "id": "4pl1zQkhnpAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 7)Explain the vanishing and exploding gradients"
      ],
      "metadata": {
        "id": "-UCncA8M2xF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing gradients occur when the gradients of the loss function with respect to the model parameters become extremely small as they propagate backward through the layers of the network during training.\n",
        "\n",
        "Exploding gradients occur when the gradients become extremely large during trainingWhen gradients become very large, the updates to the model parameters can also become very large, causing the parameters to change rapidly and making the training process unstable."
      ],
      "metadata": {
        "id": "Pl8S2ZriqFWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 8)Explain batch and layer normalization in detail."
      ],
      "metadata": {
        "id": "fNW8Hmd52xIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization normalizes each feature independently across the mini-batch.\n",
        "For each feature, the mean and standard deviation are calculated across the instances in the batch.\n",
        "This  normalizes each feature based on the distribution within the current batch.\n",
        "Itt is Generally preferred for tasks with large datasets and large batch sizes.\n",
        "May be more effective for tasks where features have similar scales.\n",
        "\n",
        "Layer normalization normalizes each of the inputs in the batch independently across all features.\n",
        "Independent of the batch size.\n",
        "Mean and standard deviation are calculated across all features for each input in the batch.\n",
        "Normalizes each input based on the distribution of its features.\n",
        "A good choice for tasks with small batch sizes or recurrent neural networks (RNNs) and transformers (sequence models) where the batch concept might be limited.\n"
      ],
      "metadata": {
        "id": "c7QhqC06X-i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9)What are regularization techniques in machine learning?(200 words)"
      ],
      "metadata": {
        "id": "6pMnZ6z52xLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Regularization techniques in machine learning are methods used to prevent overfitting and improve the performance of models by adding constraints.this will prevent the model from memorizing the training data too closely (including random noises and errrors) which leads to a model where it performs better in training data but has low accuracy and poor performance on data set. Some reularization methods are:\n",
        "\n",
        "Dropout:It randomly drops a proportion of neurons from layers(this means that the output of these neurons is set to zero during a forward pass, and their weights are not updated during backpropagation.) so that the output is not over dependepent on a certain neuron.\n",
        "\n",
        "L1 and L2 regularization adds penalty terms to the loss function based on the magnitude of the model's weights.\n",
        "\n",
        "L1(Lasso regression) regularization adds a penalty term to the loss function proportional to the absolute values of the model's weights.it is useful for feature selection.The cost function ofL1 regularization is:\n",
        "Cost=Loss+λ*(sum of all weights). where λ is regularization factor.\n",
        "\n",
        "L2(Ridge Regression)regularization adds a penalty term to the loss function proportional to the squared magnitudes of the model's weights.it prevent the weights from growing too large and thus prevent overfitting.The cost function of L2 regularization is:\n",
        "Cost= loss+λ*(sum of square of all weights)."
      ],
      "metadata": {
        "id": "5HTuam5MtFj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10)What is dropout layer and explain how it prevents overfitting?"
      ],
      "metadata": {
        "id": "EoVZoLpB3B7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout randomly drops a proportion of neurons from layers(this means that the output of these neurons is set to zero during a forward pass, and their weights are not updated during backpropagation.) so that the output is not over dependepent on a certain neuron.This prevents neurons from relying too heavily on each other.It also reduces the chances of the model to overfit the data and makes the model learn generalizable features."
      ],
      "metadata": {
        "id": "gvQR1OLq_wU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 11)Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "CMx3OcJ53CBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 and L2 regularization adds penalty terms to the loss function based on the magnitude of the model's weights.\n",
        "\n",
        "L1(Lasso regression) regularization adds a penalty term to the loss function proportional to the absolute values of the model's weights.it is useful for feature selection.The cost function ofL1 regularization is:\n",
        "Cost=Loss+λ*(sum of all weights). where λ is regularization factor.\n",
        "\n",
        "L2(Ridge Regression)regularization adds a penalty term to the loss function proportional to the squared magnitudes of the model's weights.it prevent the weights from growing too large and thus prevent overfitting.The cost function of L2 regularization is:\n",
        "Cost= loss+λ*(sum of square of all weights)."
      ],
      "metadata": {
        "id": "6LPngwMmGLWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 12)Write about validation accuracy and why we need it?"
      ],
      "metadata": {
        "id": "k5TGmHuJ3JTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation accuracy is used to analyse performance of a model. It tells us that how accurate our model can classify the data in the test dataset.It helps us to compare the performance of different models and optimizers.It also identifies overfitting of the data .It also helps in early stopping during training by monitoring the validation accuracy usually when the accuracy stops improving or starts to decline, preventing overfitting of the data.It also provides an estimate of how well the model will perform on new data."
      ],
      "metadata": {
        "id": "cQ0qzJYuHB7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q 13)What do you mean by data augmentation and explain is advantages and disadvantages in detail?"
      ],
      "metadata": {
        "id": "hCOGr8vl3JXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data augmentation is a technique to artificially expand the size and diversity of a training dataset. By applying various transformations to existing data points, data augmentation creates new variations that the model can learn from.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Improved Model Performance: With a larger and more diverse dataset created through augmentation, the model has more information to learn from. This can lead to significant improvements in the model's accuracy.\n",
        "\n",
        "Reduced Overfitting:Data augmentation helps by increasing the variety of data the model sees during training.\n",
        "\n",
        "Collecting and labeling large amounts of high-quality data can be expensive and time-consuming. Data augmentation offers a cost-effective way to virtually increase the size and diversity of your dataset\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Increased Computational Cost: Applying data augmentation techniques requires additional processing power, as each data point is transformed multiple times.\n",
        "\n",
        "Introducing Noise: If data augmentation is not implemented carefully, it can introduce unrealistic or nonsensical variations into the data."
      ],
      "metadata": {
        "id": "_FPKgJgyHtH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 14)What is transfer learning.Explain in detail? Mention various pre-trained model present in the community"
      ],
      "metadata": {
        "id": "CeYHJXKh3O1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning is a technique in machine learning where a model trained on one task is used as the starting point for a model on a second task. This is useful when the second task is similar to the first task, or when there is limited data available for the second task. We take the knowledge gained from a complex model trained on a large dataset and apply it to a new, related task therefor it helps in time saving and faster training of the model.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Better Performance: Especially for small datasets, transfer learning can significantly improve the performance of your model compared to training from scratch.\n",
        "\n",
        "Reduced Overfitting: By using pre-trained features, the model is less likely to overfit to the specific noise or quirks of your smaller dataset.\n",
        "\n",
        "some popular pre trained models present are:\n",
        "\n",
        "Computer Vision:\n",
        "\n",
        "ImageNet: A massive dataset of labeled images, often used for pre-training models for image classification, object detection, and other vision tasks.\n",
        "\n",
        "VGG16, ResNet-50: Convolutional neural network architectures pre-trained on ImageNet.\n",
        "\n",
        "Natural Language Processing (NLP):\n",
        "\n",
        "BERT, GPT-3: Large language models pre-trained on massive amounts of text data, useful for various NLP tasks like text classification, sentiment analysis, and machine translation.\n"
      ],
      "metadata": {
        "id": "XvhKUNNbNSt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 15) Explain the bias- variance tradeoff."
      ],
      "metadata": {
        "id": "m0x7E-Cg3Vzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias- variance tradeoff describes the relationship between the bias of a model and its variance.The bias-variance tradeoff arises because decreasing one component (bias or variance) often leads to an increase in the other component. Finding the right balance between bias and variance is crucial for building models that generalize well to new data.\n",
        "Bias refers to the error introduced by approximating a real-world problem with a simplified model.\n",
        "Variance refers to the variability of a model's predictions across different training datasets. A high variance model is sensitive to small fluctuations in the training data.\n",
        "Models with high bias and low variance are too simplistic and make strong assumptions about the data.These models are prone to underfitting and have poor performance on both the training and test datasets.\n",
        "Models with low bias and high variance are overly complex and fit the training data too closely.These models are prone to overfitting and have excellent performance on the training dataset but poor performance on new, unseen data."
      ],
      "metadata": {
        "id": "FepXg7_zQmuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 16)Assume you have dataset of patients who visited AIIMS(Delhi) between the period of 2018-2020.The datasets include features from CBC reports,IGE,weight,temp,etc.There problems were mainly classified into gastrointestinal problems, heart problems, diabetes and misc.\n",
        "You being a experienced ML engineer, the hospital has approached you to make a model which given these features can predict the problem that the patient is suffering.You can either train a separate neural network for each of the diseases or to train a single neural network\n",
        "with one output neuron for each disease. Which method do you prefer.Justify your answer"
      ],
      "metadata": {
        "id": "fUU9eDCa3S2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would recommend training a single neural network with one output neuron for each disease (multi-class classification) over training separate neural networks for each disease.Advantages of Single Neural Network with Multi-class Classification:\n",
        "\n",
        "Efficiency: Training and maintaining a single model is simpler and less computationally expensive than managing multiple separate models.\n",
        "\n",
        "Shared Features: Many diseases might share some underlying physiological features reflected in the data (e.g., elevated temperature for both gastrointestinal and heart problems). A single model can learn these shared features more effectively, improving overall performance.\n",
        "\n",
        "Regularization: A single model with more output neurons uses technique called L2 regularization, which penalizes overly complex models and helps prevent overfitting. This can be  beneficial with a  limited dataset."
      ],
      "metadata": {
        "id": "h317hZRyTCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 17) Assume your input data X has m observations and n0 features (shape: n0 x m), and the output layer Y has shape (n3 x m). Say that you want to have 2 hidden layers in your model: A1- (n1 x m) and A2- (n2 x m).\n",
        "\n",
        "(a) How many weight and bias matrices will you have? Find the shapes of each & verify if the dimensions of matrix multiplications are accurate.\n",
        "\n",
        "(b) If this is a Binary classification problem, what activation functions will you use for (i) the hidden layers, and (ii) the output layer. Why?\n",
        "\n",
        "(c) Repeat part (b) if this is a Multi-class classification problem.\n",
        "\n",
        "(d) Repeat part (b) if this is a Regression problem.\n",
        "\n",
        "-> Give proper reasoning for each part."
      ],
      "metadata": {
        "id": "ZRbqSHnH0Cu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) 2 Weight  Matrices and 2 Bias Matrices:\n",
        "Shapes:\n",
        "W1: (n1 x n0) - Weights for mapping input (n0 features) to hidden layer 1 (n1 neurons)\n",
        "b1: (n1 x 1) - Biases for hidden layer 1 (one bias per neuron)\n",
        "W2: (n2 x n1) - Weights for mapping hidden layer 1 (n1 features) to hidden layer 2 (n2 neurons)\n",
        "b2: (n2 x 1) - Biases for hidden layer 2 (one bias per neuron)\n",
        "\n",
        "(b) Hidden Layers: ReLU (Rectified Linear Unit)\n",
        "\n",
        "Reason: ReLU is computationally efficient, avoids the vanishing gradient problem, and works well in hidden layers for binary classification tasks. It introduces non-linearity, allowing the network to learn complex relationships.\n",
        "\n",
        "Output Layer :Sigmoid\n",
        "\n",
        "Reason: Sigmoid activation maps the output values between 0 and 1, representing the probability of belonging to the positive class in a binary classification problem.\n",
        "\n",
        "(c)Hidden Layers : ReLU\n",
        "\n",
        "Reason:  ReLU is computationally efficient, avoids the vanishing gradient problem, and works well in hidden layers for binary classification tasks. It introduces non-linearity, allowing the network to learn complex relationships.\n",
        "\n",
        "output layer: Softmax\n",
        "\n",
        "Reason: Softmax activation normalizes the outputs of the final layer into probability values between 0 and 1, representing the probability distribution across all possible classes in a multi-class classification problem. The sum of outputs from the Softmax layer will always be 1.\n",
        "\n",
        "(d) Hidden Layers: ReLU\n",
        "\n",
        "Reason: ReLU is computationally efficient, avoids the vanishing gradient problem, and works well in hidden layers for binary classification tasks. It introduces non-linearity, allowing the network to learn complex relationships.\n",
        "\n",
        "Output layer: Linear (no activation)\n",
        "\n",
        "Reason: In regression, the output represents continuous values, and we don't need a function to map them to a specific range. The linear activation allows the model to directly predict the continuous target values."
      ],
      "metadata": {
        "id": "77jLinbaT6QM"
      }
    }
  ]
}